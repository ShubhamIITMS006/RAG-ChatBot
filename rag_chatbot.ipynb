{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the PDF directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(\"Data\")\n",
    "    return document_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS-R.2.1 Minimum Qualifications for Admission to the M.S programme100\n",
      "Candidates applying for the M.S programme in one of the following areas need to have any one of101\n",
      "the minimum qualifications mentioned in the table below.102\n",
      "Area Minimum Qualifications\n",
      "Educational Qualifications Additional Qualifications\n",
      "Engineering\n",
      "B.E/ B.Tech/ 4 year online / any recog-\n",
      "nised 4 year B.sc/ 4 year BS of IITs/\n",
      "CFTIs /UGC or Master’s degree in\n",
      "a relevant discipline, or equivalent.\n",
      "301st Senate Res. No 5/2023\n",
      "or\n",
      "Associate Membership of the follow-\n",
      "ing professional bodies of the discipline,\n",
      "provided they have passed parts A and B\n",
      "of the membership examinations: The\n",
      "Institution of Engineers (India)(Civil,\n",
      "Mechanical, Electrical and Electronics,\n",
      "Electronics and Communications), The\n",
      "Aeronautical Society of India, The In-\n",
      "dian Institute of Metals, The Indian In-\n",
      "stitute of Chemical Engineers, The In-\n",
      "stitute of Electronics & Telecommunica-\n",
      "tion Engineering and other professional\n",
      "bodies approved by the Senate from time\n",
      "to time.\n",
      "or\n",
      "4 year online or any recognized 4 year\n",
      "BSc / 4 Year BS of IITs/ CFTIs/ UGC\n",
      "305th Senate Res. No 51/2023\n",
      "For Computer Science and Engi-\n",
      "neering. BSc (Maths/Stats/CS)\n",
      "+MSc (Maths/Stats/CS) holders\n",
      "from any recognized Institute/ Uni-\n",
      "versity, provided they have a valid\n",
      "GATE score (in CS/MA), or UGC-\n",
      "NET/CSIRNET/ NBHM/Inspire or\n",
      "equivalent qualification tenable for the\n",
      "year of registration, are also eligible.\n",
      "299th Senate Res. No 57/2022\n",
      "Valid GATE score\n",
      "is required for\n",
      "Regular-HTRA,\n",
      "Regular-Fellowship\n",
      "and Regular-\n",
      "Project-HTRA\n",
      "categories, except\n",
      "4 yr B.S/4 yr\n",
      "B.Sc./B.E/B.Tech\n",
      "from a Centrally\n",
      "Funded Technical\n",
      "Institute (CFTI)\n",
      "with CGPA ≥ 8.\n",
      "301st Senate Res. No 5/2023\n",
      "Page 3 of 16\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents()            ## list of tuples, where tuple contains page_content & meta_data\n",
    "print(documents[25].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the pages into smaller Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap = 200,\n",
    "        length_function = len,              # Decides how the length of the chunk is calculated. len : character count; tiktoken: token based; lambda x : x.split(): word based\n",
    "        is_separator_regex=False            # By default, the splitter uses a list of preferred string separators, eg. [\"\\n\\n\", \"\\n\", \" \", \"\"]. When false the separaters are treated as a plain string but when true split is done based on complex splitting logic.\n",
    "    )\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "## Normal seperator & Regex(Regular Expression) separators\n",
    "## Normal separators are like saying: “Split the text wherever you see this exact substring. Example: split on \"--\" means you only cut when you see two hyphens next to each other.\n",
    "## Regex separators are like saying: “Split the text wherever a pattern matches,” which can describe many possibilities compactly. Example: split on \\s+ means “any run of whitespace (spaces, tabs, newlines)” — not a fixed string, but a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='However, in the case of service officers under the control of Army / Navy / Airforce / DRDO, \n",
      "the selection will be through a central selection committee(s) with the Institute faculty serving \n",
      "on the selection committee. \n",
      " \n",
      "R.1.9 Vacancies, if required to be filled up after the admission date, will be decided by the \n",
      "Chairman, Senate, and reported to the Senate for post-facto approval. \n",
      " \n",
      "R.1.10 In all matters concerning the selection of candidates, the decision of the Chairman, Senate, or \n",
      "his / her nominee, viz. Chairman, M.Tech Admissions Committee, is final. \n",
      " \n",
      "R1.11 In addition to satisfying the conditions given in the information Brochure for M.Tech Admission \n",
      "sent along with the application forms, the selected candidates should satisfy the other \n",
      "admission requirements indicated in the offer letter of admission. Only then, they will be   \n",
      "3' metadata={'producer': 'convertonlinefree.com', 'creator': 'convertonlinefree.com', 'creationdate': '2016-04-11T11:36:51+00:00', 'moddate': '2016-04-11T17:08:52-07:00', 'source': 'Data\\\\m.tech-2015.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4'}\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents()\n",
    "chunks = split_documents(documents)             ## List of lists\n",
    "print(chunks[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Indexing of the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function is used to assign unique and tracable id to each chunk.\n",
    "\n",
    "def define_chunk_ids(chunks):\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page_label\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        # Increment the chunk index for every chunk, regardless of the page\n",
    "        current_chunk_index += 1\n",
    "\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embedding Function (illustrative)\n",
    "\n",
    "#### To create the database and to extract data by querying the database.\n",
    "\n",
    "This is saved as a python function in .py file for reuse. Here, we use `OllamaEmbeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "def get_embedding_function():\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name = \"sentence-transformers/all-miniLM-L6-v2\",\n",
    "        # trust_remote_code=True\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (0.3.27)\n",
      "Requirement already satisfied: langchain-community in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (0.3.27)\n",
      "Requirement already satisfied: pypdf in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (5.9.0)\n",
      "Requirement already satisfied: chromadb in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (1.0.15)\n",
      "Requirement already satisfied: pytest in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (8.4.1)\n",
      "Requirement already satisfied: langchain-ollama in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (0.3.6)\n",
      "Requirement already satisfied: langchain-chroma in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (0.2.5)\n",
      "Requirement already satisfied: langchain-huggingface in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (0.3.1)\n",
      "Requirement already satisfied: transformers in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (4.54.1)\n",
      "Requirement already satisfied: huggingface-hub in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (0.34.3)\n",
      "Requirement already satisfied: sentence-transformers in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (5.0.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 1)) (0.3.72)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 1)) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 1)) (0.4.10)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 1)) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 1)) (2.0.42)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 1)) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain->-r requirements.txt (line 1)) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain->-r requirements.txt (line 1)) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain->-r requirements.txt (line 1)) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 1)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 1)) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 1)) (3.2.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 2)) (3.12.15)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 2)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 2)) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain-community->-r requirements.txt (line 2)) (2.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 2)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 2)) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 2)) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirements.txt (line 2)) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 2)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->-r requirements.txt (line 2)) (1.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirements.txt (line 2)) (1.1.0)\n",
      "Requirement already satisfied: build>=1.0.3 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4)) (0.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (1.22.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (1.36.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (0.21.4)\n",
      "Requirement already satisfied: pypika>=0.48.9 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (1.74.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (0.16.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (33.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (3.11.1)\n",
      "Requirement already satisfied: httpx>=0.27.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (14.1.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from chromadb->-r requirements.txt (line 4)) (4.25.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.2 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirements.txt (line 4)) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb->-r requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: colorama>=0.4 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from pytest->-r requirements.txt (line 5)) (0.4.6)\n",
      "Requirement already satisfied: iniconfig>=1 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from pytest->-r requirements.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from pytest->-r requirements.txt (line 5)) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from pytest->-r requirements.txt (line 5)) (2.19.2)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.5.1 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langchain-ollama->-r requirements.txt (line 6)) (0.5.1)\n",
      "Requirement already satisfied: filelock in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from transformers->-r requirements.txt (line 9)) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from transformers->-r requirements.txt (line 9)) (2025.7.34)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from transformers->-r requirements.txt (line 9)) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from huggingface-hub->-r requirements.txt (line 10)) (2025.7.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 11)) (2.7.1)\n",
      "Requirement already satisfied: scikit-learn in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 11)) (1.7.1)\n",
      "Requirement already satisfied: scipy in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 11)) (1.16.1)\n",
      "Requirement already satisfied: Pillow in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from sentence-transformers->-r requirements.txt (line 11)) (11.3.0)\n",
      "Requirement already satisfied: pyproject_hooks in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: anyio in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from httpx>=0.27.0->chromadb->-r requirements.txt (line 4)) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from httpx>=0.27.0->chromadb->-r requirements.txt (line 4)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb->-r requirements.txt (line 4)) (0.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 4)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 4)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb->-r requirements.txt (line 4)) (0.26.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (2.40.3)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (0.10)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirements.txt (line 4)) (0.6.1)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain->-r requirements.txt (line 1)) (0.23.0)\n",
      "Requirement already satisfied: coloredlogs in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (25.2.10)\n",
      "Requirement already satisfied: protobuf in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (6.31.1)\n",
      "Requirement already satisfied: sympy in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 4)) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirements.txt (line 4)) (3.23.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 4)) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 4)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.36.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirements.txt (line 4)) (1.36.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb->-r requirements.txt (line 4)) (0.57b0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from rich>=10.11.0->chromadb->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirements.txt (line 4)) (0.1.2)\n",
      "Requirement already satisfied: networkx in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 11)) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 11)) (3.1.6)\n",
      "Requirement already satisfied: setuptools in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 11)) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 4)) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb->-r requirements.txt (line 4)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4)) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb->-r requirements.txt (line 4)) (15.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirements.txt (line 4)) (3.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 11)) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 11)) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\machine learning\\projects\\project-6\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 11)) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the vector Database and Enabling Auto-addition of a new file\n",
    "\n",
    "When a new file is added to the \"data\" directory, the program will detect this based on the index and add them without complete updation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from get_embedding_function import get_embedding_function\n",
    "from langchain_chroma.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  This function helps in adding document chunks to the chroma vector database\n",
    "import shutil\n",
    "CHROMA_PATH = \"chroma_new\"                                     # the directory where the Chroma database is stored or will be created.\n",
    "def add_to_chromadb(chunks: list[Document]):\n",
    "    db = Chroma(\n",
    "        collection_name= \"chunks\", persist_directory=CHROMA_PATH, embedding_function=get_embedding_function()\n",
    "    )\n",
    "\n",
    "    chunks_with_ids = define_chunk_ids(chunks)\n",
    "\n",
    "    existing_chunks = db.get(include =[])                   # nothing in include means documents, metadata & embedding won't be loaded but only ids will be loaded in vector store               \n",
    "    existing_ids = set(existing_chunks[\"ids\"])              # list of ids are converted to set for fast lookups (not the chunk id but the id that is created by default)\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    new_chunks = []\n",
    "    for chunk in chunks_with_ids:\n",
    "        if chunk.metadata[\"id\"] not in existing_ids:\n",
    "            new_chunks.append(chunk)                    ## list of chunks which will contain page_content & metadata\n",
    "        \n",
    "    if len(new_chunks):\n",
    "        print(f\"New {len(new_chunks)} documents added to the DB\")\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in new_chunks]\n",
    "        shutil.rmtree('./chroma_db', ignore_errors=True)# List of chunk ids that  we have created\n",
    "        db.add_documents(new_chunks, ids = new_chunk_ids)\n",
    "        # db.persist()\n",
    "        print(\"chunk embedded!\")\n",
    "    else:\n",
    "        print(\"No documents to add!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine Learning\\Projects\\Project-6\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of existing documents in DB: 515\n",
      "No documents to add!\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--reset\", action=\"store_true\", help=\"Reset the database.\")\n",
    "args = parser.parse_known_args()\n",
    "# if args.reset:\n",
    "#     print(\"Clearing Database\")\n",
    "#     clear_database()\n",
    "add_to_chromadb(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma.vectorstores import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "from get_embedding_function import get_embedding_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = \"chroma_new\"\n",
    "\n",
    "sys_instructions = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"You are an academic assistant chatbot for a university. \n",
    "You specialize in answering questions about the Ordinances and Regulations related to M.Tech, MS, and PhD programs. \n",
    "\n",
    "Your job is to:\n",
    "- Provide **accurate**, **clear**, and **concise** responses using the information available in the university's official ordinance documents.\n",
    "- **Stick strictly to the content** in the provided documents. If the answer is not found, say: \"I'm sorry, that information isn't available in the current document.\"\n",
    "- Explain terms in simple, student-friendly language when necessary.\n",
    "- When questions are ambiguous, **ask for clarification** instead of guessing.\n",
    "- Always maintain a **formal and helpful** tone.\n",
    "\n",
    "The document includes topics like:\n",
    "- Course structure and credits\n",
    "- Registration and thesis submission rules\n",
    "- Evaluation procedures and grading\n",
    "- Leaves and attendance\n",
    "- Program duration and extension policies\n",
    "- Comprehensive exam and academic misconduct policies\n",
    "\n",
    "You are not allowed to provide speculative advice or answer beyond what is present in the document.\n",
    "Question: \n",
    "{question}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "rag_context = HumanMessagePromptTemplate.from_template(\"Answer the question based on the following context: {context} Question: {question}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='You are an academic assistant chatbot for a university. \\nYou specialize in answering questions about the Ordinances and Regulations related to M.Tech, MS, and PhD programs. \\n\\nYour job is to:\\n- Provide **accurate**, **clear**, and **concise** responses using the information available in the university\\'s official ordinance documents.\\n- **Stick strictly to the content** in the provided documents. If the answer is not found, say: \"I\\'m sorry, that information isn\\'t available in the current document.\"\\n- Explain terms in simple, student-friendly language when necessary.\\n- When questions are ambiguous, **ask for clarification** instead of guessing.\\n- Always maintain a **formal and helpful** tone.\\n\\nThe document includes topics like:\\n- Course structure and credits\\n- Registration and thesis submission rules\\n- Evaluation procedures and grading\\n- Leaves and attendance\\n- Program duration and extension policies\\n- Comprehensive exam and academic misconduct policies\\n\\nYou are not allowed to provide speculative advice or answer beyond what is present in the document.\\nQuestion: \\n{question}\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based on the following context: {context} Question: {question}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([sys_instructions, rag_context])\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine Learning\\Projects\\Project-6\\get_embedding_function.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  return HuggingFaceEmbeddings(\n",
      "No sentence-transformers model found with name nomic-ai/nomic-bert-2048. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The repository nomic-ai/nomic-bert-2048 contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/nomic-ai/nomic-bert-2048 .\n You can inspect the repository content at https://hf.co/nomic-ai/nomic-bert-2048.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m embedding_function = \u001b[43mget_embedding_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m data_base = Chroma(collection_name=\u001b[33m\"\u001b[39m\u001b[33mchunks\u001b[39m\u001b[33m\"\u001b[39m, persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mquery_rag\u001b[39m(query_text: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# Prepare the DB.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\Projects\\Project-6\\get_embedding_function.py:4\u001b[39m, in \u001b[36mget_embedding_function\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_embedding_function\u001b[39m():\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHuggingFaceEmbeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnomic-ai/nomic-bert-2048\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\Projects\\Project-6\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:222\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    220\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    221\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\Projects\\Project-6\\.venv\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:92\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not import sentence_transformers python package. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[43msentence_transformers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\Projects\\Project-6\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:339\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    327\u001b[39m         modules, \u001b[38;5;28mself\u001b[39m.module_kwargs = \u001b[38;5;28mself\u001b[39m._load_sbert_model(\n\u001b[32m    328\u001b[39m             model_name_or_path,\n\u001b[32m    329\u001b[39m             token=token,\n\u001b[32m   (...)\u001b[39m\u001b[32m    336\u001b[39m             config_kwargs=config_kwargs,\n\u001b[32m    337\u001b[39m         )\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m         modules = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_auto_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhas_modules\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules, OrderedDict):\n\u001b[32m    353\u001b[39m     modules = OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\Projects\\Project-6\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:2061\u001b[39m, in \u001b[36mSentenceTransformer._load_auto_model\u001b[39m\u001b[34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs, has_modules)\u001b[39m\n\u001b[32m   2058\u001b[39m tokenizer_kwargs = shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {**shared_kwargs, **tokenizer_kwargs}\n\u001b[32m   2059\u001b[39m config_kwargs = shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m config_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {**shared_kwargs, **config_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m2061\u001b[39m transformer_model = \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2062\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2068\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2069\u001b[39m pooling_model = Pooling(transformer_model.get_word_embedding_dimension(), \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2070\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\Projects\\Project-6\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:87\u001b[39m, in \u001b[36mTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     85\u001b[39m     config_args = {}\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m config, is_peft_model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mself\u001b[39m._load_model(model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodel_max_length\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\Projects\\Project-6\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:152\u001b[39m, in \u001b[36mTransformer._load_config\u001b[39m\u001b[34m(self, model_name_or_path, cache_dir, backend, config_args)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[32m    150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PeftConfig.from_pretrained(model_name_or_path, **config_args, cache_dir=cache_dir), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAutoConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\Projects\\Project-6\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:1253\u001b[39m, in \u001b[36mAutoConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1251\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1252\u001b[39m         upstream_repo = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1253\u001b[39m     trust_remote_code = \u001b[43mresolve_trust_remote_code\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_local_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupstream_repo\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[32m   1258\u001b[39m     config_class = get_class_from_dynamic_module(\n\u001b[32m   1259\u001b[39m         class_ref, pretrained_model_name_or_path, code_revision=code_revision, **kwargs\n\u001b[32m   1260\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Machine Learning\\Projects\\Project-6\\.venv\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:748\u001b[39m, in \u001b[36mresolve_trust_remote_code\u001b[39m\u001b[34m(trust_remote_code, model_name, has_local_code, has_remote_code, error_message, upstream_repo)\u001b[39m\n\u001b[32m    745\u001b[39m         _raise_timeout_error(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_local_code \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    749\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m You can inspect the repository content at https://hf.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    750\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease pass the argument `trust_remote_code=True` to allow custom code to be run.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    751\u001b[39m     )\n\u001b[32m    753\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trust_remote_code\n",
      "\u001b[31mValueError\u001b[39m: The repository nomic-ai/nomic-bert-2048 contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/nomic-ai/nomic-bert-2048 .\n You can inspect the repository content at https://hf.co/nomic-ai/nomic-bert-2048.\nPlease pass the argument `trust_remote_code=True` to allow custom code to be run."
     ]
    }
   ],
   "source": [
    "embedding_function = get_embedding_function()\n",
    "data_base = Chroma(collection_name=\"chunks\", persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "def query_rag(query_text: str):\n",
    "    # Prepare the DB.\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = data_base\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "    # print(results)\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    # prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    # prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    prompt = chat_prompt.format(context=context_text, question=query_text)\n",
    "    # print(prompt)\n",
    "\n",
    "    # Use OllamaLLM for generating the response text\n",
    "    model = OllamaLLM(model=\"mistral\")\n",
    "    # model = OllamaLLM(model=\"gemma3:1b\")\n",
    "\n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    # formatted_response = f\"Response: {response_text}\"\n",
    "    # print(formatted_response)\n",
    "    return formatted_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the upgradation criteria from Mtech to PhD\n"
     ]
    }
   ],
   "source": [
    "# generated with mistral\n",
    "query_text= input(\"User: \")\n",
    "print(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine Learning\\Projects\\Project-6\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  Response:  Based on the provided document, the upgradation criteria from an M.Tech program to a Ph.D program at IITM for candidates who have already registered for an M.S/M Tech/MSc is as follows:\n",
      "\n",
      "For M.Tech:\n",
      "- Completed four courses during the first semester and obtained a CGPA ≥ 8.1\n",
      "\n",
      "For M.Sc:\n",
      "- Completed all the courses prescribed for the first 3 semesters, and with CGPA≥ 8.1\n",
      "\n",
      "For M.S:\n",
      "- CGPA ≥ 8 in the prescribed courses, with a minimum of three courses completed.\n",
      "\n",
      "The upgrade requests should be submitted within 2 years from the date of joining.\n",
      "Sources: ['Data\\\\PhD_Ordinance_updated-01-04-2024.pdf:8:238', 'Data\\\\PhD_Ordinance_updated-01-04-2024.pdf:2:193', 'Data\\\\m.tech-2015.pdf:7:31', 'Data\\\\PhD_Ordinance_updated-01-04-2024.pdf:2:194', 'Data\\\\PhD_Ordinance_updated-01-04-2024.pdf:3:216']\n"
     ]
    }
   ],
   "source": [
    "# generated with mistral\n",
    "response= query_rag(query_text)\n",
    "print(\"Response: \", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the attendance criteria to sit in the exam for Mtech students\n"
     ]
    }
   ],
   "source": [
    "# generated with mistral\n",
    "query_text= input(\"User: \")\n",
    "print(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine Learning\\Projects\\Project-6\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  Based on the provided document, M.Tech students who have less than 85% attendance in a course are not permitted to sit for the end-semester exam without the permission of the Dean Academic Courses. This criterion is specified in R.14.3. It's important to note that this rule applies only to end-semester examinations, and students who have missed sessional assessments for valid reasons can apply for a makeup examination (R.20.1). If you have any further questions or need clarification on specific terms, feel free to ask!\n",
      "Sources: ['Data\\\\m.tech-2015.pdf:14:74', 'Data\\\\m.tech-2015.pdf:11:56', 'Data\\\\m.tech-2015.pdf:2:3', 'Data\\\\m.tech-2015.pdf:9:42', 'Data\\\\m.tech-2015.pdf:13:69']\n"
     ]
    }
   ],
   "source": [
    "# generated with mistral\n",
    "response = query_rag(query_text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many days of leave can PhD take in a year\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine Learning\\Projects\\Project-6\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  Based on the provided document, PhD students are eligible for 8 days of casual leave and 15 days of vacation leave per academic year. However, it's important to note that the unutilized leave from the first year cannot be carried over to the second year. The document does not specify any specific leave policies for long leaves or medical leaves in the context of PhD programs. For further clarification on long leaves or medical leaves, I would recommend checking the university's general policies or contacting the academic administration directly.\n",
      "Sources: ['Data\\\\m.tech-2015.pdf:12:61', 'Data\\\\MS_Ordinances_20_03_2024.pdf:14:179', 'Data\\\\PhD_Ordinance_updated-01-04-2024.pdf:12:260', 'Data\\\\PhD_Ordinance_updated-01-04-2024.pdf:20:300', 'Data\\\\PhD_Ordinance_updated-01-04-2024.pdf:19:295']\n"
     ]
    }
   ],
   "source": [
    "# generated with mistral\n",
    "query_text= input(\"User: \")\n",
    "print(query_text)\n",
    "response = query_rag(query_text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the criteria of admission for Btech students at IIT Madras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine Learning\\Projects\\Project-6\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:  The criteria for admission for B.Tech students at IIT Madras includes:\n",
      "\n",
      "1. For direct admission to the M.S+Ph.D program, the top 10% of students in their 3rd year from institutions that have a specific MoU with IITM can apply. Selected students will move to IITM in their 4th year and the credits earned during the first year at IIT Madras shall be equivalent to those needed for the 4th year of the B.Tech in their parent institution. (Source: 146-152)\n",
      "\n",
      "2. Candidates should have at least 2 years of relevant work experience if they are applying as external faculty, international faculty, part-time candidates or staff members of IIT Madras. (Source: various sections from the document)\n",
      "\n",
      "3. B.Tech students from other IITs who get admitted to M.Tech with a CGPA of 8.0 are also eligible for certain programs like HTTA, but the specific criteria for these programs were not detailed in the provided document. (Source: R.28.5 and R.29.0)\n",
      "\n",
      "4. The eligibility for award of M.Tech degree includes successful completion of all core courses and project within the stipulated time. (Source: R.29.1)\n",
      "Sources: ['Data\\\\PhD_Ordinance_updated-01-04-2024.pdf:8:235', 'Data\\\\m.tech-2015.pdf:17:94', 'Data\\\\PhD_Ordinance_updated-01-04-2024.pdf:1:208', 'Data\\\\PhD_Ordinance_updated-01-04-2024.pdf:8:237', 'Data\\\\m.tech-2015.pdf:17:92']\n"
     ]
    }
   ],
   "source": [
    "query_text= input(\"User: \")\n",
    "print(query_text)\n",
    "response = query_rag(query_text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# import difflib\n",
    "\n",
    "# def normalize(text):\n",
    "#     # Lowercase, strip punctuation, and collapse whitespace\n",
    "#     text = text.lower().strip()\n",
    "#     text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "#     return ' '.join(text.split())\n",
    "\n",
    "# def evaluate_retrieval_metrics(retriever, queries, ground_truths, k=5):\n",
    "#     \"\"\"Calculate Recall@k and MRR for a set of queries.\"\"\"\n",
    "#     assert len(queries) == len(ground_truths), \"Mismatch in query and ground truth counts\"\n",
    "\n",
    "#     recall_total = 0\n",
    "#     reciprocal_ranks = []\n",
    "\n",
    "#     for query, truth in zip(queries, ground_truths):\n",
    "#         results = retriever.similarity_search_with_score(query, k=k)\n",
    "#         retrieved_texts = [doc.page_content for doc, score in results]\n",
    "\n",
    "#         normalized_truth = normalize(truth)\n",
    "#         found = False\n",
    "\n",
    "#         for rank, text in enumerate(retrieved_texts, 1):\n",
    "#             if normalized_truth in normalize(text):\n",
    "#                 recall_total += 1\n",
    "#                 reciprocal_ranks.append(1 / rank)\n",
    "#                 found = True\n",
    "#                 break\n",
    "\n",
    "#         if not found:\n",
    "#             reciprocal_ranks.append(0)\n",
    "\n",
    "#     recall_at_k = recall_total / len(queries)\n",
    "#     mrr = sum(reciprocal_ranks) / len(queries)\n",
    "\n",
    "#     print(f\"Recall@{k}: {recall_at_k:.4f}\")\n",
    "#     print(f\"MRR: {mrr:.4f}\")\n",
    "\n",
    "# def evaluate_retrieval_metrics(retriever, queries, ground_truths, k=5, threshold=0.5):\n",
    "#     assert len(queries) == len(ground_truths), \"Mismatch in query and ground truth counts\"\n",
    "\n",
    "#     recall_total = 0\n",
    "#     reciprocal_ranks = []\n",
    "\n",
    "#     for query, truth in zip(queries, ground_truths):\n",
    "#         results = retriever.similarity_search_with_score(query, k=k)\n",
    "#         normalized_truth = normalize(truth)\n",
    "\n",
    "#         found = False\n",
    "\n",
    "#         for rank, (doc, score) in enumerate(results, 1):\n",
    "#             retrieved_text = normalize(doc.page_content)\n",
    "#             similarity = difflib.SequenceMatcher(None, normalized_truth, retrieved_text).ratio()\n",
    "\n",
    "#             if similarity > threshold:\n",
    "#                 recall_total += 1\n",
    "#                 reciprocal_ranks.append(1 / rank)\n",
    "#                 found = True\n",
    "#                 break\n",
    "\n",
    "#         if not found:\n",
    "#             reciprocal_ranks.append(0)\n",
    "\n",
    "#     recall_at_k = recall_total / len(queries)\n",
    "#     mrr = sum(reciprocal_ranks) / len(queries)\n",
    "\n",
    "#     print(f\"Recall@{k}: {recall_at_k:.4f}\")\n",
    "#     print(f\"MRR: {mrr:.4f}\")\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm  # for progress bar\n",
    "\n",
    "# Load sentence embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # small and fast\n",
    "\n",
    "def semantic_similarity(text1, text2):\n",
    "    \"\"\"Compute cosine similarity between two texts using embeddings.\"\"\"\n",
    "    emb1 = embedding_model.encode(text1, convert_to_tensor=True)\n",
    "    emb2 = embedding_model.encode(text2, convert_to_tensor=True)\n",
    "    return util.pytorch_cos_sim(emb1, emb2).item()  # returns a float\n",
    "\n",
    "def evaluate_mme(retriever, queries, ground_truths, k=5, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate retrieval using semantic similarity.\n",
    "    Computes Recall@k and MRR.\n",
    "    \"\"\"\n",
    "    assert len(queries) == len(ground_truths), \"Mismatch in query and ground truth lengths\"\n",
    "\n",
    "    recall_total = 0\n",
    "    reciprocal_ranks = []\n",
    "\n",
    "    for query, ground_truth in tqdm(zip(queries, ground_truths), total=len(queries), desc=\"Evaluating\"):\n",
    "        results = retriever.similarity_search_with_score(query, k=k)\n",
    "\n",
    "        found = False\n",
    "\n",
    "        for rank, (doc, _) in enumerate(results, 1):\n",
    "            similarity = semantic_similarity(ground_truth, doc.page_content)\n",
    "\n",
    "            if similarity >= threshold:\n",
    "                recall_total += 1\n",
    "                reciprocal_ranks.append(1 / rank)\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        if not found:\n",
    "            reciprocal_ranks.append(0)\n",
    "\n",
    "    recall_at_k = recall_total / len(queries)\n",
    "    mrr = sum(reciprocal_ranks) / len(queries)\n",
    "\n",
    "    print(f\"\\nMME Evaluation:\")\n",
    "    print(f\"Recall@{k}: {recall_at_k:.4f}\")\n",
    "    print(f\"MRR: {mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:00<00:00,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MME Evaluation:\n",
      "Recall@5: 0.7500\n",
      "MRR: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What is the upgradation criteria from Mtech to PhD\",\n",
    "    \"What is the criteria for admission in MS\",\n",
    "    \"how many days of leave can PhD take in a year\",\n",
    "    \"What is the attendance criteria to sit in the exam for Mtech students\"\n",
    "]\n",
    "\n",
    "ground_truths = [\n",
    "    \"completed four courses during the first semester and obtained a CGPA ≥ 8.1\",\n",
    "    \"should possess a B.E/B.Tech degree or its equivalent from a recognized institute\",\n",
    "    \"Based on the provided document, PhD students are eligible for 8 days of casual leave and 15 days of vacation leave per academic year\",\n",
    "    \"Based on the provided document, M.Tech students who have less than 85% attendance in a course are not permitted to sit for the end-semester exam without the permission of the Dean Academic Courses. This criterion is specified in R.14.3. It's important to note that this rule applies only to end-semester examinations, and students who have missed sessional assessments for valid reasons can apply for a makeup examination (R.20.1). If you have any further questions or need clarification on specific terms, feel free to ask\"\n",
    "]\n",
    "\n",
    "evaluate_mme(retriever=data_base, queries=queries, ground_truths=ground_truths, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot with memory ready! Type 'exit' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine Learning\\Projects\\Project-6\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User tel me about foreign conference for phd\n",
      "Bot:  Based on the provided context, there is no explicit mention of a specific foreign conference for PhD students. However, it is stated that research publications arising out of the Ph.D work can be in any appropriate language (PhD-R.2.5 M.Tech+Ph.D Dual-Degree program in Engineering Design Department). This suggests that PhD scholars might have opportunities to present their research at international conferences, but the context does not provide details about such conference(s) or eligibility criteria for participation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine Learning\\Projects\\Project-6\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User anything mentioned about the Management Studies department\n",
      "Bot:  There is no direct mention in the provided context about foreign conferences for PhD students in the Management Studies department. However, it can be inferred that foreign nationals' applications may be considered without a test/interview (204), which could potentially apply to academic events as well, but this is not explicitly stated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Machine Learning\\Projects\\Project-6\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User ok exit\n",
      "Bot:  Yes, you may end the conversation now.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Create a windowed memory (last 5 turns)\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k=5,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Create a conversational RAG chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=OllamaLLM(model=\"mistral\"),\n",
    "    retriever=data_base.as_retriever(),\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Chatbot with memory ready! Type 'exit' to quit.\\n\")\n",
    "while True:\n",
    "    query = input(\"You: \")\n",
    "    if query.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    response = qa_chain.invoke({\"question\": query})\n",
    "    print(\"User\", query)\n",
    "    print(\"Bot:\", response['answer'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
